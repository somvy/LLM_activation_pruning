{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, set_seed\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# from torchao.dtypes.floatx import to_scaled_tc_floatx\n",
    "# from torchao.ops import quant_llm_linear\n",
    "\n",
    "\n",
    "def time_pytorch_function(func, input):\n",
    "    # Функция для имерения скорости расчета `func` для входа `input`\n",
    "\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(*input)\n",
    "\n",
    "    start.record()\n",
    "    func(*input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end)\n",
    "\n",
    "def print_memory():\n",
    "    # Функция измерения затраченной GPU памяти\n",
    "    device='cuda'\n",
    "    mem_allocated = torch.cuda.memory_allocated(device=device) / 1024**3\n",
    "    mem_reserved = torch.cuda.memory_allocated(device=device) / 1024**3\n",
    "    print(f\"allocated: {mem_allocated:,.2f} gb\")\n",
    "    print(f\" reserved: {mem_reserved:,.2f} gb\")\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# Load and process wikitext2 dataset\n",
    "def get_wikitext2(nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
    "    # Load test datasets\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "    trainloader = None\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "# Function to evaluate perplexity (ppl) specifically on the wikitext dataset\n",
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    nsamples = testenc.numel() // model.seqlen\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in range(0,nsamples,bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"sample {i}\")\n",
    "\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return ppl.item()\n",
    "\n",
    "\n",
    "# Function to evaluate perplexity (ppl) on a specified model and tokenizer\n",
    "def eval_ppl(model, tokenizer, device=torch.device(\"cuda:0\")):\n",
    "    # Set dataset\n",
    "    dataset = \"wikitext2\"\n",
    "    model.seqlen = 2048\n",
    "\n",
    "    # Print status\n",
    "    print(f\"evaluating on {dataset}\")\n",
    "\n",
    "    # Get the test loader\n",
    "    _, testloader = get_wikitext2(seqlen=model.seqlen, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate ppl in no grad context to avoid updating the model\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)\n",
    "    return ppl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.models.qwen3.modeling_qwen3 import Qwen3MLP\n",
    "from transformers.models.llama.modeling_llama import LlamaMLP\n",
    "from transformers.activations import ACT2FN\n",
    "from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensorCUTLASS, SparseSemiStructuredTensorCUSPARSELT\n",
    "\n",
    "class MLP_act_sp(nn.Module):\n",
    "    def __init__(self, config, backend=None):\n",
    "        super().__init__()\n",
    "        if hasattr(config, \"mlp_bias\"):\n",
    "            bias = config.mlp_bias\n",
    "        else:\n",
    "            bias = False\n",
    "        self.backend = backend\n",
    "\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size        \n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=bias)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.backend is None:\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        elif self.backend == \"cusparselt\":\n",
    "            bs, seq_len, _ = x.shape\n",
    "            x_flat = x.view(-1, self.hidden_size)\n",
    "            pruned_x = SparseSemiStructuredTensorCUSPARSELT.prune_dense_static_sort(x_flat)\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(pruned_x)) * self.up_proj(pruned_x))\n",
    "        elif self.backend == \"cutlass\":\n",
    "            bs, seq_len, _ = x.shape\n",
    "            x_flat = x.view(-1, self.hidden_size)\n",
    "            pruned_x = SparseSemiStructuredTensorCUTLASS.prune_dense_static_sort(x_flat)\n",
    "            \n",
    "            # down_proj = self.down_proj(self.act_fn(self.gate_proj(pruned_x)) * self.up_proj(pruned_x))\n",
    "            out_gate_proj = self.gate_proj(x_flat).view(bs, seq_len, -1)\n",
    "            out_up_proj = self.up_proj(x_flat).view(bs, seq_len, -1)\n",
    "            # down_proj = self.down_proj(self.act_fn(out_gate_proj) * out_up_proj)  \n",
    "\n",
    "            in_down_proj = self.act_fn(out_gate_proj) * out_up_proj\n",
    "            in_down_proj = in_down_proj.view(-1, self.intermediate_size)\n",
    "            pruned_x = SparseSemiStructuredTensorCUTLASS.prune_dense_static_sort(in_down_proj)\n",
    "            down_proj = self.down_proj(pruned_x)             \n",
    "            down_proj = down_proj.view(bs, seq_len, self.hidden_size)\n",
    "\n",
    "        return down_proj\n",
    "    \n",
    "    @classmethod\n",
    "    def from_original(cls, orig_MLP, backend=None):\n",
    "        mlp_sp = cls(orig_MLP.config, backend)\n",
    "        mlp_sp.gate_proj = orig_MLP.gate_proj\n",
    "        mlp_sp.up_proj = orig_MLP.up_proj\n",
    "        mlp_sp.down_proj = orig_MLP.down_proj\n",
    "\n",
    "        return mlp_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  25%|██▌       | 1/4 [08:17<24:51, 497.05s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.38 GiB of which 117.56 MiB is free. Process 3790346 has 174.00 MiB memory in use. Process 2714302 has 32.82 GiB memory in use. Process 3582483 has 11.26 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 339.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/LLaMA/huggingface/meta-llama-3.1-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model_path = \"/home/LLaMA/huggingface/Qwen3-14B\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# config = AutoConfig.from_pretrained(model_path)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# torch_dtype=torch.bfloat16,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:4399\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4390\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4392\u001b[0m     (\n\u001b[1;32m   4393\u001b[0m         model,\n\u001b[1;32m   4394\u001b[0m         missing_keys,\n\u001b[1;32m   4395\u001b[0m         unexpected_keys,\n\u001b[1;32m   4396\u001b[0m         mismatched_keys,\n\u001b[1;32m   4397\u001b[0m         offload_index,\n\u001b[1;32m   4398\u001b[0m         error_msgs,\n\u001b[0;32m-> 4399\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4408\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4417\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4418\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:4833\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   4832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m-> 4833\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[1;32m   4852\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:789\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    787\u001b[0m param \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 789\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[1;32m    791\u001b[0m     param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.38 GiB of which 117.56 MiB is free. Process 3790346 has 174.00 MiB memory in use. Process 2714302 has 32.82 GiB memory in use. Process 3582483 has 11.26 GiB memory in use. Of the allocated memory 10.63 GiB is allocated by PyTorch, and 339.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# model_path = \"/home/LLaMA/huggingface/Qwen3-14B\"\n",
    "model_path = \"/home/LLaMA/huggingface/meta-llama-3.1-8b\"\n",
    "\n",
    "# model_path = \"/home/LLaMA/huggingface/Qwen3-14B\"\n",
    "# config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code = True,\n",
    "    device_map = 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_module = model \n",
    "module_name_dict = {name: module for name, module in root_module.named_modules()}\n",
    "for name, module in module_name_dict.items():\n",
    "    # if isinstance(module, Qwen3MLP):\n",
    "    if isinstance(module, LlamaMLP):\n",
    "    # if isinstance(module, MLP_act_sp):\n",
    "    # if isinstance(module, torch.nn.Linear) and (name.find(\"down_proj\") != -1):\n",
    "        ind = name.rfind(\".\")\n",
    "        if ind == -1:\n",
    "            father = module_name_dict[\"\"]\n",
    "        else:\n",
    "            father = module_name_dict[name[:ind]]\n",
    "        \n",
    "        sparse_mlp = MLP_act_sp.from_original(module, backend=\"cutlass\")\n",
    "        setattr(father, name[ind + 1 :], sparse_mlp)\n",
    "        # print(f\"replace layer {name} with {q_linear}\")\n",
    "        # del module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 141\n",
      "sample 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/sparse/semi_structured.py:115: UserWarning: The PyTorch API of SparseSemiStructuredTensor is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.sparse module for further information about the project.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 50\n",
      "sample 100\n"
     ]
    }
   ],
   "source": [
    "ppl = eval_ppl(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.43801879882812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl #up_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.568115234375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl #gate_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4142.78076171875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl #down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136190.21875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl #down_proj + up_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.645155906677246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl #orig 1m 44s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289076 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 141\n",
      "sample 0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 44.38 GiB of which 32.62 MiB is free. Process 3893576 has 15.30 GiB memory in use. Process 4170050 has 29.01 GiB memory in use. Of the allocated memory 14.98 GiB is allocated by PyTorch, and 16.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppl \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36meval_ppl\u001b[0;34m(model, tokenizer, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Evaluate ppl in no grad context to avoid updating the model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 112\u001b[0m     ppl_test \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppl_wikitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ppl_test\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36meval_ppl_wikitext\u001b[0;34m(model, testenc, bs, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mreshape(j\u001b[38;5;241m-\u001b[39mi, model\u001b[38;5;241m.\u001b[39mseqlen)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Shift logits and labels for next token prediction\u001b[39;00m\n\u001b[1;32m     76\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m lm_logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:315\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    312\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    313\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 315\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    319\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    320\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     79\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 80\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 44.38 GiB of which 32.62 MiB is free. Process 3893576 has 15.30 GiB memory in use. Process 4170050 has 29.01 GiB memory in use. Of the allocated memory 14.98 GiB is allocated by PyTorch, and 16.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "ppl = eval_ppl(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.3795051574707"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl # 1m 40s up_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7053.65673828125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl # 1m 44s gate_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.703990936279297"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl # 1m 44s down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 146\n",
      "sample 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Linear.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppl \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ppl\n",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m, in \u001b[0;36meval_ppl\u001b[0;34m(model, tokenizer, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Evaluate ppl in no grad context to avoid updating the model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 112\u001b[0m     ppl_test \u001b[38;5;241m=\u001b[39m \u001b[43meval_ppl_wikitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ppl_test\n",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m, in \u001b[0;36meval_ppl_wikitext\u001b[0;34m(model, testenc, bs, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mreshape(j\u001b[38;5;241m-\u001b[39mi, model\u001b[38;5;241m.\u001b[39mseqlen)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Shift logits and labels for next token prediction\u001b[39;00m\n\u001b[1;32m     76\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m lm_logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:850\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    846\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    847\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:576\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    565\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    566\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m         position_embeddings,\n\u001b[1;32m    574\u001b[0m     )\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:305\u001b[0m, in \u001b[0;36mQwen3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    308\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mMLP_act_sp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m     out_gate_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x_flat)\u001b[38;5;241m.\u001b[39mview(bs, seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     out_up_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(pruned_x)\u001b[38;5;241m.\u001b[39mview(bs, seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_gate_proj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_up_proj\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# in_down_proj = self.act_fn(out_gate_proj) * out_up_proj\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# in_down_proj = in_down_proj.view(-1, self.intermediate_size)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# pruned_x = SparseSemiStructuredTensorCUTLASS.prune_dense_static_sort(in_down_proj)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# down_proj = self.down_proj(pruned_x)             \u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# down_proj = down_proj.view(bs, seq_len, self.hidden_size)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "ppl = eval_ppl(model, tokenizer)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.703990936279297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_(model, int4_weight_only(layout=MarlinSparseLayout()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 146\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n"
     ]
    }
   ],
   "source": [
    "ppl = eval_ppl(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.645155906677246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.01717758178711"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "torch.cuda.synchronize()\n",
    "end.record()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark with BT=1024, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=dense\n",
      "Running benchmark with BT=1024, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cutlass\n",
      "Running benchmark with BT=1024, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cusparselt\n",
      "Running benchmark with BT=2048, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=dense\n",
      "Running benchmark with BT=2048, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cutlass\n",
      "Running benchmark with BT=2048, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cusparselt\n",
      "Running benchmark with BT=4096, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=dense\n",
      "Running benchmark with BT=4096, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cutlass\n",
      "Running benchmark with BT=4096, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cusparselt\n",
      "Running benchmark with BT=8192, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=dense\n",
      "Running benchmark with BT=8192, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cutlass\n",
      "Running benchmark with BT=8192, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cusparselt\n",
      "Running benchmark with BT=16384, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=dense\n",
      "Running benchmark with BT=16384, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cutlass\n",
      "Running benchmark with BT=16384, INPUT_SIZE=14336, OUTPUT_SIZE=4096, dtype=torch.float16 provider=cusparselt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Structural:\n",
      "        BT     dense   cutlass  cusparselt\n",
      "0   1024.0  0.001089  0.001058    0.001860\n",
      "1   2048.0  0.002433  0.001961    0.003739\n",
      "2   4096.0  0.004668  0.004002    0.006679\n",
      "3   8192.0  0.009262  0.008747    0.016191\n",
      "4  16384.0  0.016173  0.020165    0.038043\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensorCUTLASS, SparseSemiStructuredTensorCUSPARSELT\n",
    "from pathlib import Path\n",
    "\n",
    "def test_speed(func, _iter):\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(_iter):\n",
    "        start.record()\n",
    "        func()\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end) / 1000)        \n",
    "\n",
    "    total_time = sum(times) / _iter\n",
    "\n",
    "    return total_time\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    [\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"BT\"],\n",
    "            x_vals=[2**i for i in range(10, 15)], # 1024, 2048, 4096\n",
    "            xlabel=\"B x T\",\n",
    "            line_arg=\"provider\",\n",
    "            line_vals=[\"dense\", \"cutlass\", \"cusparselt\"],\n",
    "            line_names=[\"dense\", \"cutlass\", \"cusparselt\"],\n",
    "            styles=[\n",
    "                (\"blue\", \"solid\"),\n",
    "                (\"orange\", \"solid\"),\n",
    "                (\"green\", \"solid\"),\n",
    "            ],\n",
    "            ylabel=\"Time (s)\",\n",
    "            plot_name=\"Semi-Structural\",\n",
    "            args={\"INPUT_SIZE\": 14336, \"OUTPUT_SIZE\": 4096, \"dtype\": torch.float16},\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "def bench_speed_semi_structural(BT, INPUT_SIZE, OUTPUT_SIZE, provider, dtype, device=\"cuda\"):\n",
    "    print(\n",
    "        f\"Running benchmark with BT={BT}, INPUT_SIZE={INPUT_SIZE}, OUTPUT_SIZE={OUTPUT_SIZE}, dtype={dtype} provider={provider}\"\n",
    "    )\n",
    "\n",
    "    _input = torch.randn(BT, INPUT_SIZE, requires_grad=False, dtype=dtype, device=device)\n",
    "    weight = torch.randn(INPUT_SIZE, OUTPUT_SIZE, requires_grad=False, dtype=dtype, device=device)\n",
    "\n",
    "    def fwd():\n",
    "        if provider == \"dense\":\n",
    "            return _input @ weight\n",
    "        elif provider == \"cutlass\":\n",
    "            pruned_input = SparseSemiStructuredTensorCUTLASS.prune_dense_static_sort(_input)\n",
    "            return pruned_input._mm(weight)\n",
    "        elif provider == \"cusparselt\":\n",
    "            pruned_input = SparseSemiStructuredTensorCUSPARSELT.prune_dense_static_sort(_input)\n",
    "            return pruned_input._mm(weight)\n",
    "        \n",
    "    time = test_speed(fwd, _iter=100)\n",
    "    return time\n",
    "\n",
    "df = bench_speed_semi_structural.run(show_plots=True, print_data=True, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BT</th>\n",
       "      <th>dense</th>\n",
       "      <th>cutlass</th>\n",
       "      <th>cusparselt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.001739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2048.0</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.002683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.005187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8192.0</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.011041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16384.0</td>\n",
       "      <td>0.016410</td>\n",
       "      <td>0.017807</td>\n",
       "      <td>0.030562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BT     dense   cutlass  cusparselt\n",
       "0   1024.0  0.001104  0.000885    0.001739\n",
       "1   2048.0  0.002059  0.001751    0.002683\n",
       "2   4096.0  0.004168  0.003772    0.005187\n",
       "3   8192.0  0.008211  0.008859    0.011041\n",
       "4  16384.0  0.016410  0.017807    0.030562"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_folder = Path(\"/home/Quantization/yandex_seminar/speculative_decoding/data/\")\n",
    "file_name = 'A40_llama_4096x14336' + '.txt'\n",
    "path_to_file = path_to_folder / file_name \n",
    "df[0].to_csv(path_to_file, index=False)\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_memory(func, _iter):\n",
    "    total_mem = []\n",
    "\n",
    "    for _ in range(_iter):\n",
    "        torch.cuda.memory.reset_peak_memory_stats()\n",
    "        func()\n",
    "        mem = torch.cuda.max_memory_allocated() / (2**20)\n",
    "        total_mem.append(mem)\n",
    "\n",
    "    return sum(total_mem) / len(total_mem)\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    [\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"BT\"],\n",
    "            x_vals=[2**i for i in range(10, 13)], # 1024, 2048, 4096\n",
    "            xlabel=\"B x T\",\n",
    "            line_arg=\"provider\",\n",
    "            line_vals=[\"liger\", \"huggingface\"],\n",
    "            line_names=[\"Liger\", \"Hugging Face\"],\n",
    "            styles=[\n",
    "                (\"blue\", \"solid\"),\n",
    "                (\"orange\", \"solid\"),\n",
    "            ],\n",
    "            ylabel=\"GPU memory usage (MB)\",\n",
    "            plot_name=\"fused-linear-cross-entropy-memory-benchmark\",\n",
    "            args={\"H\": 4096, \"V\": 128256, \"dtype\": torch.float32},\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "def bench_memory_cross_entropy(BT, H, V, provider, dtype, device=\"cuda\"):\n",
    "    print(\n",
    "        f\"Running benchmark with BT={BT}, H={H}, V={V}, dtype={dtype} provider={provider}\"\n",
    "    )\n",
    "    torch_lm_head_ce = TorchLMHeadCE(H=H, V=V, dtype=dtype).to(device)\n",
    "    liger_lm_head_ce = LigerLMHeadCE(H=H, V=V, dtype=dtype).to(device)\n",
    "\n",
    "    _input = torch.randn(BT, H, requires_grad=True, dtype=dtype, device=device)\n",
    "    target = torch.randint(V, (BT, 1), dtype=torch.long, device=device).squeeze(1)\n",
    "\n",
    "    def fwd():\n",
    "        if provider == \"liger\":\n",
    "            return liger_lm_head_ce(_input, target)\n",
    "        elif provider == \"huggingface\":\n",
    "            return torch_lm_head_ce(_input, target)\n",
    "\n",
    "    def full():\n",
    "        y = fwd()\n",
    "        y.backward()\n",
    "\n",
    "    mem = test_memory(full, _iter=10)\n",
    "    return mem\n",
    "\n",
    "\n",
    "bench_memory_cross_entropy.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensorCUTLASS\n",
    "\n",
    "def time_pytorch_function(func, input):\n",
    "    # Функция для имерения скорости расчета `func` для входа `input`\n",
    "\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(*input)\n",
    "\n",
    "    start.record()\n",
    "    func(*input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end)\n",
    "\n",
    "input = torch.zeros((5120, 5120)).half().cuda()\n",
    "input = torch.zeros((17408)).half().cuda()\n",
    "# input = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0992640033364296"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# mat1 = torch.randn(5120, 5120).half().cuda()\n",
    "# mat2 = torch.randn(5120, 5120).half().cuda()\n",
    "\n",
    "mat1 = torch.randn(128, 5120).half().cuda()\n",
    "mat2 = torch.randn(5120, 5120).half().cuda()\n",
    "\n",
    "for _ in range(5):\n",
    "    mat1 @ mat2\n",
    "\n",
    "start.record()\n",
    "mat1 @ mat2\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/sparse/semi_structured.py:115: UserWarning: The PyTorch API of SparseSemiStructuredTensor is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.sparse module for further information about the project.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09523200243711472"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for _ in range(5):\n",
    "    mat1._mm(mat2)\n",
    "\n",
    "start.record()\n",
    "mat1._mm(mat2)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sparse.admm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mЧто такое компьютер?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:3431\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3428\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3431\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:850\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    846\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    847\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:553\u001b[0m, in \u001b[0;36mQwen3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[1;32m    556\u001b[0m all_hidden_states \u001b[38;5;241m=\u001b[39m () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_rope_utils.py:87\u001b[0m, in \u001b[0;36mdynamic_rope_update.<locals>.wrapper\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongrope\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     86\u001b[0m     longrope_frequency_update(\u001b[38;5;28mself\u001b[39m, position_ids, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrope_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py:341\u001b[0m, in \u001b[0;36mQwen3RotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    339\u001b[0m device_type \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice_type, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):  \u001b[38;5;66;03m# Force float32\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m (\u001b[43minv_freq_expanded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids_expanded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    342\u001b[0m     emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    343\u001b[0m     cos \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mcos() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_scaling\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 50\n",
    "use_cache =True\n",
    "prompt = \"Что такое компьютер?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(torch.device(\"cuda:0\"))\n",
    "output = model.generate(input_ids, max_new_tokens=max_new_tokens, use_cache=use_cache)\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text: str, tokenizer) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot who always responds as superhuman intelligence AI\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Some queries for LLMs\n",
    "texts = [\n",
    "    \"What are the pros/cons of ChatGPT vs Open Source LLMs?\",\n",
    "    \"Write an email to a new client to offer a subscription for a paper supply for 1 year.\",\n",
    "    \"I have $10,000 USD for investment. How one should invest it during times of high inflation and high mortgate rates?\",\n",
    "    \"Write a function in python that calculates the square of a sum of two numbers.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchao.dtypes.floatx import to_scaled_tc_floatx\n",
    "from torchao.ops import quant_llm_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_weight = torch.randn(1024, 512).cuda().half()\n",
    "# ebits, mbits = 3, 2\n",
    "ebits, mbits = 3, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the weight. this will quantize the weight to FP6 and pack it in a special\n",
    "# layout for tensor cores. refer to paper for more details.\n",
    "fp6_weight, scales = to_scaled_tc_floatx(fp32_weight, ebits, mbits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "FP4 E3M0 is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fp16_act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mquant_llm_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mebits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmbits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp6_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (1, 1024)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchao/ops.py:124\u001b[0m, in \u001b[0;36mquant_llm_linear\u001b[0;34m(EXPONENT, MANTISSA, _in_feats, _weights, _scales, splitK)\u001b[0m\n\u001b[1;32m    119\u001b[0m compute_capability \u001b[38;5;241m=\u001b[39m cached_compute_capability()\n\u001b[1;32m    120\u001b[0m torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    121\u001b[0m     compute_capability \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_llm_linear requires sm7.5+ GPU architecture, but current device has sm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompute_capability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m )\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_llm_linear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEXPONENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMANTISSA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_in_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_scales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplitK\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_ops.py:723\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: FP4 E3M0 is not supported."
     ]
    }
   ],
   "source": [
    "fp16_act = torch.randn(1, 512).cuda().half()\n",
    "outputs = quant_llm_linear(ebits, mbits, fp16_act, fp6_weight, scales)  # shape (1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 21.8281,  33.8125, -27.0938,  ...,  29.4688,  19.3750,  -8.3984]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
